model_list:
  # Free / trial-first providers
  - model_name: groq/llama3-70b-8192
    litellm_provider: groq
    api_key: ${GROQ_API_KEY}
  - model_name: groq/llama3-8b-8192
    litellm_provider: groq
    api_key: ${GROQ_API_KEY}
  - model_name: google/gemini-1.5-flash
    litellm_provider: google
    api_key: ${GEMINI_API_KEY}
  - model_name: huggingface/meta-llama/Llama-3.2-3B-Instruct
    litellm_provider: huggingface
    api_key: ${HUGGINGFACE_API_KEY}
  - model_name: openrouter/meta-llama/llama-3.1-8b-instruct
    litellm_provider: openrouter
    api_key: ${OPENROUTER_API_KEY}

  # OpenAI Models
  - model_name: openai/gpt-4o-mini
    litellm_provider: openai
    api_key: ${OPENAI_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.00015
    rpm: 10000
    tpm: 200000
  - model_name: openai/gpt-4o
    litellm_provider: openai
    api_key: ${OPENAI_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.005
    rpm: 5000
    tpm: 100000
  - model_name: openai/gpt-3.5-turbo
    litellm_provider: openai
    api_key: ${OPENAI_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0005
    rpm: 10000
    tpm: 200000

  # Groq Models
  - model_name: groq/llama-3.1-70b
    litellm_provider: groq
    api_key: ${GROQ_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0001
    rpm: 30000
    tpm: 600000
  - model_name: groq/llama-3.1-8b
    litellm_provider: groq
    api_key: ${GROQ_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.00005
    rpm: 30000
    tpm: 600000

  # Mistral Models
  - model_name: mistral/mistral-small-latest
    litellm_provider: mistral
    api_key: ${MISTRAL_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0002
    rpm: 20000
    tpm: 400000
  - model_name: mistral/mistral-large-latest
    litellm_provider: mistral
    api_key: ${MISTRAL_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0008
    rpm: 10000
    tpm: 200000

  # Anthropic Models
  - model_name: anthropic/claude-3-haiku
    litellm_provider: anthropic
    api_key: ${ANTHROPIC_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.00025
    rpm: 10000
    tpm: 200000
  - model_name: anthropic/claude-3-sonnet
    litellm_provider: anthropic
    api_key: ${ANTHROPIC_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.003
    rpm: 5000
    tpm: 100000

  # OpenRouter Models (Alternative aggregator)
  - model_name: openrouter/meta-llama/llama-2-70b-chat
    litellm_provider: openrouter
    api_key: ${OPENROUTER_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0007
    rpm: 10000
    tpm: 200000
  - model_name: openrouter/google/palm-2-chat-bison
    litellm_provider: openrouter
    api_key: ${OPENROUTER_API_KEY}
    max_tokens: 4000
    temperature: 0.7
    cost_per_token: 0.0004
    rpm: 10000
    tpm: 200000

routers:
  # Free/trial-first routing
  - router_name: free-first
    routing_strategy: random_weighted
    models:
      - name: groq/llama3-70b-8192
        weight: 60
      - name: groq/llama3-8b-8192
        weight: 25
      - name: google/gemini-1.5-flash
        weight: 15
      - name: huggingface/meta-llama/Llama-3.2-3B-Instruct
        weight: 10
      - name: openrouter/meta-llama/llama-3.1-8b-instruct
        weight: 5

  # Fast and cheap routing for general chat
  - router_name: fast-cheap
    routing_strategy: latency
    models:
      - name: groq/llama-3.1-70b
        weight: 0.4
      - name: openai/gpt-4o-mini
        weight: 0.3
      - name: mistral/mistral-small-latest
        weight: 0.2
      - name: anthropic/claude-3-haiku
        weight: 0.1

  # High quality routing for complex tasks
  - router_name: high-quality
    routing_strategy: cost
    models:
      - name: openai/gpt-4o
        weight: 0.4
      - name: anthropic/claude-3-sonnet
        weight: 0.3
      - name: mistral/mistral-large-latest
        weight: 0.2
      - name: openai/gpt-4o-mini
        weight: 0.1

  # Ultra fast routing for quick responses
  - router_name: ultra-fast
    routing_strategy: latency
    models:
      - name: groq/llama-3.1-8b
        weight: 0.5
      - name: openai/gpt-3.5-turbo
        weight: 0.3
      - name: groq/llama-3.1-70b
        weight: 0.2

  # Fallback routing with OpenRouter
  - router_name: fallback
    routing_strategy: cost
    models:
      - name: openrouter/meta-llama/llama-2-70b-chat
        weight: 0.6
      - name: openrouter/google/palm-2-chat-bison
        weight: 0.4

# General settings
general_settings:
  master_key: ${LITELLM_MASTER_KEY}
  database_url: ${DATABASE_URL}
  enable_pre_call_checks: true
  enable_post_call_metrics: true
  success_callback: []
  failure_callback: []
  max_budget: 100.0
  budget_duration: "1d"
  alerting: ["email"]
  alerting_threshold: 0.8

# Logging and monitoring
litellm_settings:
  set_verbose: true
  drop_params: true
  add_function_to_prompt: true
  api_base: null
  cache: true
  cache_ttl: 300
  cache_ttl_redis: 3600
  redis_host: ${REDIS_HOST:-localhost}
  redis_port: ${REDIS_PORT:-6379}
  redis_password: ${REDIS_PASSWORD}
  use_redis: true
  telemetry: true
  disable_spend_logs: false

# Rate limiting and throttling
router_settings:
  routing_strategy: latency
  num_retries: 3
  timeout: 60
  retry_delay: 1
  cooldown_time: 10
  max_queued_requests: 100
  enable_pre_call_checks: true
  enable_dynamic_routing: true
  routing_fallback: "fallback"

# Advanced routing
routing_settings:
  enable_router_failover: true
  enable_router_retries: true
  enable_router_cooldowns: true
  enable_router_load_balancing: true
  enable_router_health_checks: true
  health_check_interval: 30
  health_check_timeout: 10
  health_check_failure_threshold: 3

# Cost tracking
cost_tracking:
  enabled: true
  track_cost_per_model: true
  track_cost_per_user: true
  track_cost_per_team: true
  budget_alerts: true
  cost_anomaly_detection: true

# Security
security:
  enable_auth: true
  enable_rbac: true
  enable_api_key_rotation: true
  enable_ip_whitelisting: true
  enable_rate_limiting: true
  max_requests_per_minute: 100
  max_requests_per_hour: 1000

# Health check
health_check:
  enabled: true
  interval: 30
  timeout: 10
  failure_threshold: 3
  success_threshold: 2

# Performance optimization
performance:
  enable_connection_pooling: true
  max_connections: 100
  connection_timeout: 30
  enable_compression: true
  enable_http2: true
  enable_keep_alive: true
